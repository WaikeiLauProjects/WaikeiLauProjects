---
title: "Group H Capstone project"
author: "Waikei"
date: "2023-03-20"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#git add --all
#git commit -m "update code"
#git push
#git push --set-upstream origin TrainTestSplit


library(tsibble)
library(lubridate)
library(dplyr)
library(tidyr)
library(sqldf)
library(reticulate)
library(readr)
library(zoo)
library(data.table)


```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r load, include=FALSE}

#~~~Read Tempurature~~~#
Temp_NSW <- read.csv("../data/temperature_nsw.csv") %>%
  na.omit() %>%
  filter(TEMPERATURE >=-500) %>%
  mutate(DATETIME = ymd_hms(DATETIME)) 
# which(is.na(Temp_NSW$DATETIME))

#~~~Read Demand~~~#
Demand_NSW <- read_csv("../data/totaldemand_nsw.csv.zip", show_col_types = FALSE) %>% #read from github
  na.omit() %>% distinct() %>% #remove duplicates and NA values
  mutate(DATETIME = ymd_hms(DATETIME)) %>%
  left_join(Temp_NSW[c(-2)], by=c('DATETIME')) %>% #Add Temperature (To Demand)#
  rename(Temp='TEMPERATURE') %>%
  fill(Temp) %>% select(-2) %>%
  filter(DATETIME>='2019-06-15') #keep 3 years data

```

```{r}
#~~~Create 30min trend DF from Demand~~~#
# Demand_NSW <- Demand_NSW[!Demand_NSW$Solar_MW %in% c("NULL",NA),]

Trend_Data <- Demand_NSW %>% 
  group_by(DATETIME=ceiling_date(DATETIME, "30 mins")) %>% summarise(
  AvG30_Demand = mean(TOTALDEMAND, na.rm = TRUE), #~~average demand since last half hour
  AvG30_Temp = mean(Temp, na.rm = TRUE) #~~average Temperature since last half hour
  )%>% as.data.frame()

```


```{r}

#~~~generate rolling averages from 5min demand ~~~#

Demand_NSW <- Demand_NSW %>% mutate(
  Day_Demand = rollmean(TOTALDEMAND, k=288, fill=NA, align='right'), #~~rolling avg demand 24hours
  Day_Temp = rollmean(Temp, k=288, fill=NA, align='right'), #~~rolling avg Temperature 24hours
  Week_Demand = rollmean(TOTALDEMAND, k=2016, fill=NA, align='right'), #~~rolling avg demand 7 days
  Week_Temp = rollmean(Temp, k=2016, fill=NA, align='right') #~~rolling avg Temp 7 days
  )

```


```{r}

#~~~join rolling demand with 30min demand~~~#

Demand_NSW <- Demand_NSW[minute(Demand_NSW$DATETIME)%%30==0,]

Trend_Data <- Trend_Data %>% left_join(Demand_NSW, by=c('DATETIME'))

print(Trend_Data[4225:4235,1:2])
print(subset(Demand_NSW[,1:3], duplicated(DATETIME)|duplicated(DATETIME, fromLast=TRUE)))


```


```{r}

#~~~ create lagged differences ~~~#

Trend_Data$Half_Lag_Demand <- c(NA, head(Trend_Data$AvG30_Demand, -1))
Trend_Data$Half_Lag_Temp <- c(NA, head(Trend_Data$AvG30_Temp, -1))
Trend_Data$Day_Lag_Demand <- c(rep(NA, 48), head(Trend_Data$Day_Demand, -48))
Trend_Data$Day_Lag_Temp <- c(rep(NA, 48), head(Trend_Data$Day_Temp, -48))
Trend_Data$Week_Lag_Demand <- c(rep(NA, 336), head(Trend_Data$Week_Demand, -336))
Trend_Data$Week_Lag_Temp <- c(rep(NA, 336), head(Trend_Data$Week_Temp, -336))

Trend_Data$Half_Diff_Demand <- Trend_Data$AvG30_Demand - Trend_Data$Half_Lag_Demand
Trend_Data$Half_Diff_Temp <- Trend_Data$AvG30_Temp - Trend_Data$Half_Lag_Temp
Trend_Data$Day_Diff_Demand <- Trend_Data$Day_Demand - Trend_Data$Day_Lag_Demand
Trend_Data$Day_Diff_Temp <- Trend_Data$Day_Temp - Trend_Data$Day_Lag_Temp
Trend_Data$Week_Diff_Demand <- Trend_Data$Week_Demand - Trend_Data$Week_Lag_Demand
Trend_Data$Week_Diff_Temp <- Trend_Data$Week_Temp - Trend_Data$Week_Lag_Temp

```

```{r}

#~~~Add Season (To 30min Forecast)~~~#
#Forecast_Data$season <- ifelse(months(Forecast_Data$DATETIME) %in% month.name[c(12,1,2)], 4,
#                               ifelse(months(Forecast_Data$DATETIME) %in% month.name[c(3,4,5)], 3,
#                                      ifelse(months(Forecast_Data$DATETIME) %in% month.name[c(6,7,8)], 2,1)))


#~~~Add Day of week (To 30min Forecast)~~~#
Trend_Data$Weekday<- lubridate::wday(Trend_Data$DATETIME, label = FALSE)
Trend_Data$Month <- format(as.Date(Trend_Data$DATETIME, format="%d/%m/%Y"),"%m")
Trend_Data$Time <- as.integer(format(as.POSIXct(Trend_Data$DATETIME), format = "%H%M"))


#~~~Add Holidays (To 30min Forecast)~~~#
PublicHolidays <- as.data.frame(tsibble::holiday_aus(year = 2010:2023, state = "NSW"))
PublicHolidays$day <- ifelse(wday(PublicHolidays$date) == 1, 1, ifelse(wday(PublicHolidays$date) == 7, 2,0))
PublicHolidays$Obs <- PublicHolidays$date + PublicHolidays$day
Trend_Data$PubHol <- ifelse(as.Date(Trend_Data$DATETIME,format = "%y-%m-%d") %in% PublicHolidays$Obs, 1, 0)




```


```{r}

#~~~Optional variables

#~~~Add Daily rainfall (To 30min Forecast)~~~#
#Rainfall_data <- read.csv.sql("../data/IDCJAC0009_066137_1800_Data Rainfall.csv",
#                              sql="select * from file where Year >= 2010") %>% 
#  na.omit(Rainfall_data)
#Rainfall_data$Date <- as.Date(with(Rainfall_data,paste(Year, Month, Day, sep="-")), "%Y-%m-%d")
#Rainfall_data <- Rainfall_data[c(9,6)]
#colnames(Rainfall_data) = c("DATETIME", "Rainfall_mm_Day")
#Forecast_Data <- Forecast_Data %>% left_join(Rainfall_data,by=c('DATETIME')) %>% fill(Rainfall_mm_Day)

#~~~Add Daily Solar (To 30min Forecast)~~~#
#Solar_data <- read.csv.sql("../data/IDCJAC0016_066137_1800_Data Solar.csv",
#                       sql="select * from file where Year >= 2010")
#Solar_data <- na.omit(Solar_data)
#Solar_data$Date <- as.Date(with(Solar_data,paste(Year, Month, Day, sep="-")), "%Y-%m-%d")
#Solar_data <- Solar_data[c(7,6)]
#colnames(Solar_data) = c("DATETIME", "Total_Solar_Day")
#Forecast_Data <- Forecast_Data %>% left_join(Solar_data,by=c('DATETIME')) %>% fill(Total_Solar_Day) %>% na.omit()

```


```{r}

#~~~Create T+N target outputs for reinforcement learning~~~#
name= paste("T+",1:336, sep="")
change= paste("Diff+",1:336, sep="")

Trend_Data <- na.omit(Trend_Data)

for(i in 1:336){
  Trend_Data[[name[i]]] <- c(tail(Trend_Data$AvG30_Demand, -i), rep(NA, i))
}

for(i in 1:336){
  Trend_Data[[change[i]]] <- Trend_Data[[name[i]]] - Trend_Data$AvG30_Demand
}

Forecast_Data <- Trend_Data[c(1,16:25,362:697)]

split <-ymd_hms("2021-08-01 00:00:00")

#~~~Setting up axis for later plots~~~#
train_actuals <- Trend_Data[Trend_Data$DATETIME<split,]%>%select(2)
test_actuals <- Trend_Data[Trend_Data$DATETIME==split,]%>%select(26:361)
test_axis <- seq(split, ymd_hms("2021-08-08 00:00:00"), by = "30 min")

print(Forecast_Data)

#~~~Split into train test set~~~#
train <- Forecast_Data[Forecast_Data$DATETIME<split,]
test <- Forecast_Data[Forecast_Data$DATETIME>=split,]


```

```{r}

#~~~Run this to install python packages for the first time~~~#
#repl_python()
#py_install('pandas')
#py_install('numpy')
#py_install('matplotlib')
#py_install('seaborn')

```



```{python}

#~~~MUST RESTART R SESSION TO REFRESH PYTHON ENVIRONMENT~~~#
#py_Forecast_30m.iloc[0:0]

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import RegressorChain
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV
import shap
import time
import pickle

```

```{python}
#~~~create Train Test split for time series~~~#


train_x, train_y, test_x, test_y = r.train.set_index('DATETIME').iloc[:,:10], r.train.set_index('DATETIME').iloc[:,10:], r.test.set_index('DATETIME').iloc[:,:10], r.test.set_index('DATETIME').iloc[:,10:]

#test.iloc[-337:-336,:10] , test.iloc[-337:-336,10:]

```

```{python}
#~~ the TimeSeriesSplit component of below code was adapted from https://towardsdatascience.com/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1

#~~ it provides the necessary modifications to standard Gridsearch to be applicable to time series data


p_search = { 
    'n_estimators': [100, 500, 1000],
    'max_depth' : [5, 10]
}

test_model = RandomForestRegressor(min_samples_leaf = 50, oob_score =True)
tscv = TimeSeriesSplit(n_splits=4)

start = time.time()
gsearch = GridSearchCV(estimator=test_model, cv=tscv, param_grid=p_search, scoring='neg_mean_absolute_percentage_error')
gsearch.fit(train_x, train_y)
stop = time.time()

print(f"Grid Search time: {stop - start}s")

best_score = gsearch.best_score_
best_model = gsearch.best_estimator_

print(best_model)


```


```{python}

'''
model1 = RandomForestRegressor(n_estimators=500, max_depth = 10, min_samples_leaf = 50, oob_score =True)

start = time.time()
model1.fit(train_x, train_y)
stop = time.time()

print(f"Model 1 Training time: {stop - start}s")
'''

```
```{python}

pickle.dump(model1, open('../model/rf_model.sav', 'wb'))

loaded_model = pickle.load(open('../model/rf_model.sav', 'rb'))

pred_y = pd.DataFrame(loaded_model.predict(test_x), columns = r.change).set_index(test_y.index)

```

```{python}

imp = loaded_model.feature_importances_
features = train_x.columns
indices = np.argsort(imp)
plt.title('Feature Importances')
plt.barh(range(len(indices)), imp[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

```

```{r}

pred_y = py$pred_y
colnames(pred_y) <- name
pred_y_7day <- pred_y[1,]
ref_xy <- Trend_Data[Trend_Data$DATETIME %in% ymd_hms(row.names(pred_y_7day)),]

#~~~retrend the data

for(i in 1:336){
  pred_y[,i] <- pred_y_7day[,i] + ref_xy$AvG30_Demand
}

APE <- data.frame(matrix(0, nrow = nrow(pred_y_7day)))

#~~~find APE

for(i in 1:336){
  APE[[name[i]]] <- abs(pred_y_7day[[name[i]]] - ref_xy[[name[i]]])/ref_xy[[name[i]]]
}

APE <- APE[c(-1)]

#~~~find cMAPE

cMAPE<-data.frame(matrix(0, nrow = ncol(pred_y_7day)))

for(i in 1:length(APE)){
  cMAPE[c(i),]<-sum(colSums(APE[1:i], na.rm = TRUE))/sum(colSums(!is.na(APE[1:i])))
}

rownames(cMAPE)<-name

colnames(cMAPE)<-"CMAPE"

plot.ts(cMAPE, col = "blue", main = "CMAPE vs Forecasting Horizon")

print(cMAPE)

```


```{r}

#write.csv(pred_y, "../data/RFprediction.csv", row.names=TRUE)

```

```{python}

resid = (train_y - loaded_model.oob_prediction_)

lowq = resid.quantile(0.05, axis = 0)
highq = resid.quantile(0.95, axis = 0)

#print(lowq)
#print(highq)

```


```{r}

lq <- t(as.data.frame(py$lowq))
hq <- t(as.data.frame(py$highq))

colnames(lq) <- name
colnames(hq) <- name

lq <- rbind(lq, pred_y)
hq <- rbind(hq, pred_y)

for(i in 2:nrow(lq)){
  lq[i,] <- lq[i,] + lq[1,]
  hq[i,] <- hq[i,] + hq[1,]
}

lq <- lq[-1,]
hq <- hq[-1,]


```

```{python}

#fig, ax = plt.subplots()
plt.figure(figsize = (18,7))

plt.grid(alpha=0.5)

plt.plot(train_x.index[-672:], r.train_actuals.iloc[-672:], label = "Training observations (truncated)")

plt.plot(r.test_axis, r.test_actuals, color = "blue", label = "Test observations", ls="dashed")

plt.plot(r.test_axis,r.pred_y.iloc[0],color="purple", label = "RF Demand forecast")

plt.fill_between(r.test_axis, r.lq.iloc[0], r.hq.iloc[0], color="purple", alpha=0.5, label = "RF 90% forecast inverval")

#ax.xaxis.set_major_formatter(DateFormatter("%m-%d"))

plt.legend(fontsize=5)
plt.margins(x=0)
plt.show()

```


```{r}

ref_y <- ref_xy %>% select(26:361)

cover <- na.omit(as.data.frame((ref_y>=lq) & (ref_y<=hq)))

col_cover <- as.data.frame(colMeans(cover))
print(col_cover)

```

```{r}

ARIMA_forecast <- read.csv("../data/ARIMA_forecast_one_year.csv") %>% select(-1) %>% 
  mutate(DATETIME = seq(ymd_hms("2021-08-01 00:30:00"), ymd_hms("2022-08-01 00:00:00"), by = "30 min")) %>% relocate(DATETIME, .before=Point.Forecast) %>%
  mutate(Demand = ifelse(DATETIME %in% Demand_NSW$DATETIME, Demand_NSW$TOTALDEMAND, NA),
         T = Point.Forecast - Demand) %>% select(1,8) %>% 
  left_join(Forecast_Data[,1:11], by=c('DATETIME'='DATETIME')) %>% select(-2,2)

#for(i in 1:48){
#  ARIMA_forecast[[name[i]]] <- c(tail(ARIMA_forecast$T, -i), rep(NA, i))
#}

```

```{python}
#~~~create Train Test split for time series~~~#

py_Forecast = r.ARIMA_forecast.set_index('DATETIME') #.sort_index()

split = "2022-02-01 00:00:00"

steps= 1

train2, test2 = py_Forecast.loc[:split,:], py_Forecast.loc[split:,:]

train_x2, train_y2, test_x2, test_y2 = train2.iloc[:,:-steps], train2.iloc[:,-steps:], test2.iloc[:,:-steps], test2.iloc[:,-steps:]

```

```{python}

model2 = RandomForestRegressor(n_estimators=500, max_depth = 10, min_samples_leaf = 50)

start = time.time()
model2.fit(train_x2, train_y2.values.ravel())
stop = time.time()

print(f"Model 2 Training time: {stop - start}s")

```
```{python}

pred_y2 = pd.DataFrame(model2.predict(test_x2), columns = test_y2.columns).set_index(test_y2.index)

```

```{r}

pred_y2 = py$pred_y2
test_y2 <- py$test_y2

APE2 <- abs(pred_y2 - test_y2)/test_y2

cMAPE2<-list()

for(i in 1:length(APE2)){
  cMAPE2[i]=sum(colSums(APE2[1:i], na.rm = TRUE))/sum(colSums(!is.na(APE2[1:i])))
  #cMAPE[i+1]=(sum(APE[[name[i]]])+as.numeric(unlist(cMAPE[i])))/(i*nrow(APE))
  #print(cat(i, sum(APE[[name[i]]]), as.numeric(unlist(cMAPE[i])), i*nrow(APE)))
}

```


```{r}

Model_Switch <- read.csv("../data/ARIMA_forecast_one_year.csv") %>% select(-1) %>% 
  mutate(DATETIME = 
           seq(ymd_hms("2021-08-01 00:30:00"), ymd_hms("2022-08-01 00:00:00"), by = "30 min"),
         Demand = ifelse(DATETIME %in% Demand_NSW$DATETIME, Demand_NSW$TOTALDEMAND, NA),
         RF_pred = pred_y[ymd_hms(row.names(pred_y)) %in% ARIMA_forecast$DATETIME,1],
         AR_Sigma = (Point.Forecast - Demand),
         RF_Sigma = (RF_pred - Demand),
         Best_30m_model = ifelse(abs(AR_Sigma)>abs(RF_Sigma),"RF","ARIMA"),
         Best_30m_forecast = ifelse(abs(AR_Sigma)>abs(RF_Sigma),RF_pred,Point.Forecast),
         ARIMA = ifelse(Best_30m_model == "ARIMA", 1,0),
         RF = ifelse(Best_30m_model == "RF", 1,0),
         RS_AR_day = rollsumr(ARIMA,k=48,fill=NA),
         RS_RF_day = rollsumr(RF,k=48,fill=NA),
         Best_24h_model = ifelse((RS_AR_day)>(RS_RF_day),"ARIMA","RF"),
         RS_AR_Wk = rollsumr(ARIMA,k=336,fill=NA),
         RS_RF_Wk = rollsumr(RF,k=336,fill=NA),
         Best_Wk_model = ifelse((RS_AR_Wk)>(RS_RF_Wk),"ARIMA","RF")) %>%
  relocate(DATETIME, .before=Point.Forecast) %>%
  relocate(Demand, .before=Point.Forecast) %>%
  rename(AR_pred=Point.Forecast) %>% select(-4,-5,-6,-7,-9,-10,-13,-14,-15,-16,-18,-19)

#

#%>% select(1,2,7,8)

#%>% mutate(T = c(tail(ARIMA_forecast$T+1, -1), NA))

#for(i in 1:48){
#  ARIMA_forecast[[name[i]]] <- c(tail(ARIMA_forecast$T, -i), rep(NA, i))
#}

```

```{r}

write.csv(ARIMA_forecast, "../data/Totalprediction.csv", row.names=TRUE)

```





